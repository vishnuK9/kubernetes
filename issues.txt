########## ISSUES #########
# Kubernetes: expired certificate
	Each node within the Kubernetes cluster contains a config file for running kubelet ... /etc/kubernetes/kubelet.conf ... and this file is auto-generated by kubeadm. During this auto-generation, kubeadm uses /etc/kubernetes/ca.key to create a node-specific file, /etc/kubernetes/kubelet.conf, within which are two very important pieces ... client-certificate-data and client-key-data. My original thought process led me to believe that I needed to find the corresponding certificate file & key file, renew those files, convert both to base64, and use those values within kubelet.conf files across the cluster ... this thinking was not correct.

	Instead, the fix was to use kubeadm to regenerate kubectl.conf on all nodes, as well as admin.conf, controller-manager.conf, and scheduler.conf on the cluster's master node. You'll need /etc/kubernetes/pki/ca.key on each node in order for your config files to include valid data for client-certificate-data and client-key-data.

	Pro tip: make use of the --apiserver-advertise-address parameter to ensure your new config files contain the correct IP address of the node hosting the kube-apiserver service
	
Renew kubernetes pki after expired:

# NodePort service is not externally accessible via `port` number :
	curl <node-ip>:<node-port>        # curl <node-ip>:31000
	curl <service-ip>:<service-port>  # curl <svc-ip>:8090
	curl <pod-ip>:<target-port>       # curl <pod-ip>:80
	curl service-name:node-port
	kubectl describe service and make sure the labels match with desired the pods labels
	check if the pod is in running and checking the number of restarts. do describe pod and check kubectl logs of the pod.
	check db service and db pod logs

# Persistent Volume not found issues:
	wrong storage class used, miss match in Storage class

# kubelet stopped working on worker nodes :
	- Kubelet kube apiserver certificates expired.
	
# etcd component not in running state after restore, need a restart. Delete the etcd pod , kubernetes will restore it

# ImagePullBackOff : due to invalid image tag , or invalid permissions
	how to Debug :
		kubectl describe <podname>, if iamge is not successfully pulled, container wont be create and started which will cause ht error.
		events like repository not found or no pull access
		insufficient scope : authorisation failed
		wrong tag or use full qualified image name
		ensure your secrets and credentials are correct and they have access to the requried repo
# ErrImagePull :
# RegistryUnavailable :
# CrashLoopBackOff : 
	- after your image is pulled successfully, you notice that it immediately ran into some error and casued crashLoopBackoff. Your runtime configuration is not working. Application in not able to find the config values. Liveness probe failue will also lead to crashllopbackoff.
# KillContainerError
# OOMKilled : 
	- application is leaking memory.
	- senior will send the threaddump using jstack to the developer and he will analyse and create better application.
	- containerIq tool to log the OOM 
	- exit code 127
	
# Pod in Pending stage :
	- Due to not enough resource quota on your namespace.
	- Due to not enough resource quota on your node.
	- kubectl describe deploy 
	- kubectl get events --sort-by=.metadata.creationTimestamp
	- volume node affinity conflict

# node disk pressure:
	- significantly most likely to run into this due to logs building up. kubernetes save logs of any running container and most recently exited container. If you have a long-running container with lot of logs, they may build up enough that it overloads the capacity of the node disk.
	- way to check it is using du command, by running daemon set on each node whcih executes teh du command.
	- from the output if you see its necessary application data, then you need to increase the node size or talk with developer
	-  or u may find that application that have produced a lot of files are no longer needed. in this case we can delete the files. cannot restart the pod to delete it data, because that works on only ephemeral volumes not persistent volumes.

# node not ready :
	- lack of resources on the node, if there are lot of processes running taking too much of resources.
	- kubectl describe nodes will show you about memory pressure, disk pressure, pid pressure under the condition section to see if resources are mission on the 	node.
	- kubelet crashes or stops , it cannot communiicate to api server and marke node as not ready.
	- if all the conditions are unkown, then it means kubelet is down
	- misconfigure networking on the machine, under condition section check if NetworkUnavailable is true
	
# control-plane failure :
	- check pods in kubesystem ns
	- check the logs of the kube-apiserver component 
	- /var/log/kube-apiserver.log
	- /var/log/kube-scheduler.log
	- /var/log/kube-controller-manager.log
	- check if kublete is running
	- check if kubelet has right certificate and certificate is not expired
	- check for top node command to analyse ram and cpu
	- check disk df -h
	- 
	
##### How to Troubleshoot and Address Liveness / Readiness probe failure #####
# OBJECTIVE Identify and troubleshoot Readiness or Liveliness probes that report services as unhealthy intermittently.
# This is a known issue and can be classified into two categories, connection refused and client timeout. 
# Both categories are shown in the procedure below. The commands in this procedure assume the user is logged into either a master or worker non-compute node 
# 1. Troubleshoot a refused connection.
# kubectl get events -A | grep -i unhealthy | grep "connection refused"
# This may occur if the health check ran when the pod was being terminated. To confirm this is the case, check that the pod no longer exists. 
# If that is true, disregard this unhealthy event.
# 2. Troubleshoot a client timeout
# kubectl get events -A | grep -i unhealthy | grep "Client.Timeout\|DeadlineExceeded"
# This may occur if the health check did not respond within the specified timeout. To confirm that the service is healthy, check the health using the curl command.
# curl -i http://10.45.0.20:15020/app-health/cray-bos/livez
	
# Pod in 'CrashLoopBackOff' State - 'Readiness\Liveness probe failed: Get http://:8082/actuator/health: dial tcp :8082: connect: connection refused
# Why were the endpoints unavailable for the readiness\liveness probes?
# Review the machine resources in Grafana, it was evident that the machine resources were insufficient (RAM maxed out) so the pod (and its associated probes) were not able to check the endpoint.
# Access the pod and wait until it crashes again, kubectl exec management-96449b57b-x8swv -it -- bash , 
# we see the following message as the session is terminated: /opt/sisense/management# command terminated with exit code 137
# Exit code 137 is an error code signaling that the application arrived at an out of memory condition and the 'docker stop' command was sent by Kubernetes to the container.
# We can run the command: docker inspect $(CONTAINER_ID)
# journalctl -r -k | grep -i -e memory -e oom 
# Using the 'describe' command, we can clearly see that the container failed because OOM with exit code 137:
# Containers:
#   management:
#     ...
#     State:
#       Reason:   CrashLoopBackOff
#     Last State:
#       Reason:   Error
#       Exit Code: 137
